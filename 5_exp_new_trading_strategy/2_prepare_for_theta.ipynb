{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import numpy as np\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def aggregate_log_return(df, bucket):\n",
    "    # Define the mapping of buckets to their respective minutes\n",
    "    bucket_dict = {\n",
    "        '1min': 1,\n",
    "        '5min': 5,\n",
    "        '10min': 10,\n",
    "        '30min': 30,\n",
    "        '1h': 60,\n",
    "        '3h': 60*3,\n",
    "        '5h': 60*5,\n",
    "        '1d': 60*6.5, \n",
    "        '2d': 60*6.5*2,\n",
    "        '7d': 60*6.5*7,\n",
    "        '14d': 60*6.5*14,\n",
    "        '21d': 60*6.5*21,\n",
    "        '30d': 60*6.5*30\n",
    "    }\n",
    "    \n",
    "    # Validate the provided bucket\n",
    "    if bucket not in bucket_dict:\n",
    "        raise ValueError(\"Invalid bucket. Please choose from '1min', '5min', '10min', '30min', '1h', '1d', '2d', '7d', '14d', '21d', '30d'.\")\n",
    "    \n",
    "    # No processing needed for 1-minute intervals\n",
    "    if bucket == '1min':\n",
    "        return df\n",
    "    \n",
    "    # Number of rows to aggregate\n",
    "    n = int(bucket_dict[bucket])\n",
    "    \n",
    "    # Save date and time columns\n",
    "    dates = df['date'].values\n",
    "    times = df['time'].values\n",
    "    \n",
    "    # Drop date and time columns for calculation\n",
    "    data = df.drop(columns=['date', 'time']).values\n",
    "    \n",
    "    # Add 1 to all values\n",
    "    data += 1\n",
    "    \n",
    "    # Determine the new number of rows\n",
    "    new_row_count = len(data) // n\n",
    "    \n",
    "    # Reshape the data to (new_row_count, n, num_columns)\n",
    "    reshaped_data = data[:new_row_count * n].reshape(new_row_count, n, -1)\n",
    "    \n",
    "    # Multiply along the second axis and subtract 1\n",
    "    aggregated_data = reshaped_data.prod(axis=1) - 1\n",
    "    \n",
    "    # Extract the corresponding date and time for the new rows\n",
    "    new_dates = dates[(np.arange(new_row_count) + 1) * n - 1]\n",
    "    new_times = times[(np.arange(new_row_count) + 1) * n - 1]\n",
    "    \n",
    "    # Create the new DataFrame\n",
    "    aggregated_df = pd.DataFrame(aggregated_data, columns=df.columns[2:])\n",
    "    aggregated_df['date'] = new_dates\n",
    "    aggregated_df['time'] = new_times\n",
    "    \n",
    "    # Reorder columns to put Date and Time first\n",
    "    cols = ['date', 'time'] + [col for col in aggregated_df.columns if col not in ['date', 'time']]\n",
    "    aggregated_df = aggregated_df[cols]\n",
    "    \n",
    "    return aggregated_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../0_data_preprocessing/log_returns_1min_252.csv')\n",
    "df.iloc[:,2:] = np.exp(df.iloc[:,2:])-1\n",
    "\n",
    "bucket = '1d'\n",
    "lookback_window = 10\n",
    "method = 'levy_area'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "result_dir = f'./'\n",
    "file_path = os.path.join(result_dir, f'returns_bucket{bucket}_lookback_window{lookback_window}_{method}.pkl')\n",
    "leaders_followers_file = os.path.join(result_dir, f'leaders_followers_bucket{bucket}_lookback_window{lookback_window}_{method}.pkl')\n",
    "\n",
    "data = aggregate_log_return(df, bucket)\n",
    "data['datetime'] = pd.to_datetime(data['date'] + ' ' + data['time'])\n",
    "data.set_index('datetime', inplace=True)\n",
    "data.drop(columns=['date', 'time'], inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dic = pd.read_pickle('./leaders_followers_bucket1d_lookback_window10_levy_area.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "average_returns = []\n",
    "\n",
    "for idx in range(len(dic['t'])):\n",
    "    row_number = dic['t'][idx] \n",
    "    leaders = dic['leaders'][idx] \n",
    "    selected_data = data.iloc[row_number][leaders]\n",
    "    average_return = selected_data.mean()\n",
    "    average_returns.append(average_return)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('leaders_average_returns_over_t_window10.pkl', 'wb') as file:\n",
    "    pickle.dump(average_returns, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "leadlag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
